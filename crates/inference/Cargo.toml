[package]
name = "cortex-inference"
version.workspace = true
edition.workspace = true
description = "Distributed LLM inference engine with tensor parallelism"

[features]
default = []
llama = []  # LLaMA model support
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]

[dependencies]
cortex-core = { path = "../core" }
cortex-grid = { path = "../grid" }

# Candle ML framework (Rust-native)
candle-core = "0.8"
candle-nn = "0.8"
candle-transformers = "0.8"

# Model loading
hf-hub = "0.3"
tokenizers = "0.20"

# Async/networking
tokio = { workspace = true }
futures = { workspace = true }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }
bincode = "1.3"

# Utils
tracing = { workspace = true }
thiserror = { workspace = true }
byteorder = "1.5"
blake3 = { workspace = true }
half = "2.3"
